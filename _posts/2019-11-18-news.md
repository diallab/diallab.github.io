---
title: 이재웅 박사과정 ICDM 2019 국제 학술대회 논문 게재
layout: post
date: '2019-11-18 14:30:58'
categories:
- news
---

<p style="display: flex; justify-content: space-evenly;">
	<img style="display: flex;" src="/post_images/2019-11-18_news_image1.png">
	<img style="display: flex;" src="/post_images/2019-11-18_news_image2.png">
</p>

이종욱 교수 연구실([DIAL Lab](https://diallab.github.io/)), 이재웅 박사과정의 논문이 데이터마이닝 분야 최우수학회인 IEEE International Conference on Data Mining (ICDM) 2019에 게재 및 발표되었다. 이재웅 박사과정의 논문인 “Collaborative Distillation for Top-N Recommendation”은 딥러닝 기반 추천 모델의 정확도를 유지하며 모델을 압축하기 위한 방법에 대한 논문이며, 채택률 9.08%인 정규 논문으로 채택되었다.

논문은 기존의 이미지 분류에서 사용되는 모델 압축 방법인 지식 증류(Knowledge Distillation)을 추천 시스템에 적용하는 방법을 제시하였으며, 추천 시스템이 가진 3가지 문제, 1. 사용자가 선호하는 상위 N개의 항목을 다른 항목보다 더 중요하게 여겨야 하는 “순위 문제”, 2. 많은 항목 중 사용자가 실제 평가하는 항목이 적어서 생기는 “데이터 희소성 문제”, 그리고 3. 사용자가 평가하지 않은 항목이 사용자가 선호하지 않는 항목인지 선호하지만 아직 평가되지 않은 항목인지 모르는 “데이터 모호성 문제”를 해결하며 지식 증류 방법을 적용하였다.

구체적으로, 순위 문제는 순위 기반 샘플링 기법을 적용하여 해결하였으며, 데이터 희소성 문제를 완화하기 위해, 기존 추천 시스템의 손실 함수를 수정하여 사용자의 평가가 강조되도록 하였다. 그리고 데이터 모호성 문제를 기존에 미리 학습된 모델이 예측한 사용자의 선호도를 이용하여 해결하였으며, 위의 3가지 문제 해결 방법으로 구성된 새로운 지식 증류 기법을 제안하였다.

제안 방법은 기존의 추천 모델에 독립적으로 적용될 수 있으며, 논문에서는 기존 추천 모델에 제안 방법을 적용하여, 정확도의 손실 없이 기존 모델 대비 10%의 모델 크기를 갖고 약 7배 빠른 수행 시간을 갖는 모델을 만들었다. 본 연구에서 제시한 방법을 실제 서비스에 적용하면, 기존 서비스 제공 시간을 크게 줄일 수 있을 것으로 기대된다.